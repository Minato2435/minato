{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714017595220,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"},"user_tz":-330},"id":"G0jQAf-4aLqF","outputId":"0e92e990-d7d3-4fea-80b0-bffb3fc90c0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'study', '.']\n","Lemmas: ['Natural', 'Language', 'Processing', 'be', 'a', 'fascinating', 'field', 'of', 'study', '.']\n","\n","Dependency Parsing:\n","Natural compound Language PROPN []\n","Language compound Processing PROPN [Natural]\n","Processing nsubj is AUX [Language]\n","is ROOT is AUX [Processing, field, .]\n","a det field NOUN []\n","fascinating amod field NOUN []\n","field attr is AUX [a, fascinating, of]\n","of prep field NOUN [study]\n","study pobj of ADP []\n",". punct is AUX []\n"]}],"source":["import spacy\n","\n","# Load English tokenizer, tagger, parser, NER, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text for analysis\n","text = \"Natural Language Processing is a fascinating field of study.\"\n","\n","# Process the text with spaCy\n","doc = nlp(text)\n","\n","# Extracting tokens and lemmatization\n","tokens = [token.text for token in doc]\n","lemmas = [token.lemma_ for token in doc]\n","print(\"Tokens:\", tokens)\n","print(\"Lemmas:\", lemmas)\n","\n","# Dependency parsing\n","print(\"\\nDependency Parsing:\")\n","for token in doc:\n","    print(token.text, token.dep_, token.head.text, token.head.pos_,\n","          [child for child in token.children])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksEllFuxaKCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714020873500,"user_tz":-330,"elapsed":2197,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"6154ad39-3cfb-49e1-f7c0-002b279a9d1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Analyzing Feedback 1: 'The product is amazing! I love the quality.'\n","Tokens: ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n","Lemmas: ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n","\n","Dependency Parsing:\n","The det product NOUN []\n","product nsubj is AUX [The]\n","is ROOT is AUX [product, amazing, !]\n","amazing acomp is AUX []\n","! punct is AUX []\n","I nsubj love VERB []\n","love ROOT love VERB [I, quality, .]\n","the det quality NOUN []\n","quality dobj love VERB [the]\n",". punct love VERB []\n","\n","Analyzing Feedback 2: 'The customer service was terrible, very disappointed.'\n","Tokens: ['The', 'customer', 'service', 'was', 'terrible', ',', 'very', 'disappointed', '.']\n","Lemmas: ['the', 'customer', 'service', 'be', 'terrible', ',', 'very', 'disappointed', '.']\n","\n","Dependency Parsing:\n","The det service NOUN []\n","customer compound service NOUN []\n","service nsubj was AUX [The, customer]\n","was ROOT was AUX [service, disappointed, .]\n","terrible amod disappointed ADJ []\n",", punct disappointed ADJ []\n","very advmod disappointed ADJ []\n","disappointed acomp was AUX [terrible, ,, very]\n",". punct was AUX []\n","\n","Analyzing Feedback 3: 'Great experience overall, highly recommended.'\n","Tokens: ['Great', 'experience', 'overall', ',', 'highly', 'recommended', '.']\n","Lemmas: ['great', 'experience', 'overall', ',', 'highly', 'recommend', '.']\n","\n","Dependency Parsing:\n","Great amod experience NOUN []\n","experience nsubj recommended VERB [Great]\n","overall advmod recommended VERB []\n",", punct recommended VERB []\n","highly advmod recommended VERB []\n","recommended ROOT recommended VERB [experience, overall, ,, highly, .]\n",". punct recommended VERB []\n","\n","Analyzing Feedback 4: 'The delivery was late, very frustrating.'\n","Tokens: ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']\n","Lemmas: ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']\n","\n","Dependency Parsing:\n","The det delivery NOUN []\n","delivery nsubj was AUX [The]\n","was ROOT was AUX [delivery, frustrating, .]\n","late advmod frustrating ADJ []\n",", punct frustrating ADJ []\n","very advmod frustrating ADJ []\n","frustrating acomp was AUX [late, ,, very]\n",". punct was AUX []\n"]}],"source":["import spacy\n","\n","# Load English tokenizer, tagger, parser, NER, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample customer feedback data\n","customer_feedback = [\n","    \"The product is amazing! I love the quality.\",\n","    \"The customer service was terrible, very disappointed.\",\n","    \"Great experience overall, highly recommended.\",\n","    \"The delivery was late, very frustrating.\"\n","]\n","\n","def analyze_feedback(feedback):\n","    for idx, text in enumerate(feedback, start=1):\n","        print(f\"\\nAnalyzing Feedback {idx}: '{text}'\")\n","        doc = nlp(text)\n","        tokens = [token.text for token in doc]\n","        lemmas = [token.lemma_ for token in doc]\n","        print(\"Tokens:\", tokens)\n","        print(\"Lemmas:\", lemmas)\n","        print(\"\\nDependency Parsing:\")\n","        for token in doc:\n","            print(token.text, token.dep_, token.head.text, token.head.pos_,\n","                  [child for child in token.children])\n","\n","if __name__ == \"__main__\":\n","    analyze_feedback(customer_feedback)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pbDju4BEaafZ","outputId":"60e3d3bd-25b5-422e-882d-45b2a62a8be3"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n"]},{"name":"stdout","output_type":"stream","text":["the principal\n","the principal plants\n","the principal plants .\n","the principal plants . Friend\n","the principal plants . Friend ,\"\n","the principal plants . Friend ,\" said\n","the principal plants . Friend ,\" said her\n","the principal plants . Friend ,\" said her his\n","the principal plants . Friend ,\" said her his brethren\n","the principal plants . Friend ,\" said her his brethren ,\n","the principal plants . Friend ,\" said her his brethren , he\n","the principal plants . Friend ,\" said her his brethren , he away\n","the principal plants . Friend ,\" said her his brethren , he away thy\n","the principal plants . Friend ,\" said her his brethren , he away thy love\n","the principal plants . Friend ,\" said her his brethren , he away thy love the\n","the principal plants . Friend ,\" said her his brethren , he away thy love the yet\n","the principal plants . Friend ,\" said her his brethren , he away thy love the yet dost\n","the principal plants . Friend ,\" said her his brethren , he away thy love the yet dost thou\n","the principal plants . Friend ,\" said her his brethren , he away thy love the yet dost thou shouldest\n","the principal plants . Friend ,\" said her his brethren , he away thy love the yet dost thou shouldest fall\n"]}],"source":["#2nd\n","import nltk\n","import random\n","nltk.download('punkt')\n","nltk.download('gutenberg')\n","words = nltk.corpus.gutenberg.words()\n","bigrams = list(nltk.bigrams(words))\n","starting_word = \"the\"\n","generated_text = [starting_word]\n","for _ in range(20):\n","  possible_words = [word2 for (word1, word2) in bigrams if word1.lower() == generated_text[-1].lower()]\n","  next_word = random.choice(possible_words)\n","  generated_text.append(next_word)\n","  print(' '.join(generated_text))"]},{"cell_type":"code","source":["#2d cs\n","\n","import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","class EmailAutocompleteSystem:\n","    def __init__(self):\n","        self.model_name = \"gpt2\"\n","        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n","        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n","\n","    def generate_suggestions(self, user_input, context):\n","        input_text = f\"{context} {user_input}\"\n","        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1,no_repeat_ngram_size=2)\n","        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n","        suggestions = generated_text.split()[len(user_input.split()):]\n","        return suggestions\n","\n","if __name__ == \"__main__\":\n","    autocomplete_system = EmailAutocompleteSystem()\n","    email_context = \"Subject: Discussing Project Proposal\\nHi [Recipient],\"\n","    while True:\n","        user_input = input(\"Enter your sentence (type 'exit' to end): \")\n","        if user_input.lower() == 'exit':\n","            break\n","        suggestions = autocomplete_system.generate_suggestions(user_input, email_context)\n","        if suggestions:\n","            print(\"Autocomplete Suggestions:\", suggestions)\n","        else:\n","            print(\"No suggestions available.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meoxu-rvduG1","executionInfo":{"status":"ok","timestamp":1714018678744,"user_tz":-330,"elapsed":94364,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"42856468-955e-4125-a4b7-f20c9458b3f3"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your sentence (type 'exit' to end): hello lanj\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Autocomplete Suggestions: ['Project', 'Proposal', 'Hi', '[Recipient],', 'hello', 'lanj.', \"I'm\", 'a', 'developer', 'at', 'the', 'project.', \"It's\", 'a', 'project', 'that', \"I've\", 'been', 'working', 'on', 'for', 'a', 'while', 'now.', 'The', 'project', 'is', 'a', 'simple,', 'simple', 'project,', 'but']\n","Enter your sentence (type 'exit' to end): hi\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Autocomplete Suggestions: ['Discussing', 'Project', 'Proposal', 'Hi', '[Recipient],', 'hi.', \"I'm\", 'a', 'developer', 'at', 'a', 'company', 'that', 'is', 'working', 'on', 'a', 'project', 'called', 'Project', 'PROJECT.', \"It's\", 'a', 'small', 'project', 'that', \"I've\", 'been', 'working', 'with', 'for', 'a', 'while.', 'The', 'project']\n","Enter your sentence (type 'exit' to end): i'm a developer\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Autocomplete Suggestions: ['Proposal', 'Hi', '[Recipient],', \"i'm\", 'a', 'developer', 'at', 'a', 'company', 'that', 'is', 'working', 'on', 'a', 'project', 'called', 'Project', 'PROJECT.', \"I'm\", 'interested', 'in', 'the', 'idea', 'of', 'a', 'new', 'way', 'to', 'create', 'a', 'web', 'application', 'that', 'can', 'be', 'used']\n","Enter your sentence (type 'exit' to end): exit\n"]}]},{"cell_type":"code","source":["#3rd\n","import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","# Load the 20 Newsgroups dataset\n","categories = ['sci.med', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n","newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n","# Split the data into training and testing sets\n","X_train = newsgroups_train.data\n","X_test = newsgroups_test.data\n","y_train = newsgroups_train.target\n","y_test = newsgroups_test.target\n","# Create a pipeline with TF-IDF vectorizer and LinearSVC classifier\n","model = make_pipeline(\n","TfidfVectorizer(),\n","LinearSVC()\n",")\n","# Train the model\n","model.fit(X_train, y_train)\n","# Predict labels for the test set\n","predictions = model.predict(X_test)\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, predictions)\n","print(\"Accuracy:\", accuracy)\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, predictions))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NatdcQk6eYgl","executionInfo":{"status":"ok","timestamp":1714018743807,"user_tz":-330,"elapsed":23721,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"2c6268f0-c2da-4dc9-8119-ed109dbb1e68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9504823151125402\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.97      0.93       389\n","           1       0.96      0.91      0.94       396\n","           2       0.98      0.94      0.96       394\n","           3       0.98      0.98      0.98       376\n","\n","    accuracy                           0.95      1555\n","   macro avg       0.95      0.95      0.95      1555\n","weighted avg       0.95      0.95      0.95      1555\n","\n"]}]},{"cell_type":"code","source":["#3 cs\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the 20 Newsgroups dataset as a proxy for customer support emails\n","newsgroups = fetch_20newsgroups(subset='all', categories=['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles'])\n","\n","# Prepare data and target labels\n","X = newsgroups.data\n","y = newsgroups.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train the LinearSVC classifier\n","classifier = LinearSVC()\n","classifier.fit(X_train, y_train)\n","\n","# Predict labels for the test set\n","predictions = classifier.predict(X_test)\n","\n","# Evaluate the classifier\n","accuracy = accuracy_score(y_test, predictions)\n","print(\"Accuracy:\", accuracy)\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, predictions, target_names=newsgroups.target_names))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbmBGfJIepQ_","executionInfo":{"status":"ok","timestamp":1714019061818,"user_tz":-330,"elapsed":2566,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"974478c4-c31b-4a1b-ac57-76e7d767ba1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9479034307496823\n","\n","Classification Report:\n","                          precision    recall  f1-score   support\n","\n","comp.sys.ibm.pc.hardware       0.92      0.92      0.92       183\n","   comp.sys.mac.hardware       0.93      0.92      0.93       205\n","               rec.autos       0.97      0.97      0.97       210\n","         rec.motorcycles       0.97      0.98      0.97       189\n","\n","                accuracy                           0.95       787\n","               macro avg       0.95      0.95      0.95       787\n","            weighted avg       0.95      0.95      0.95       787\n","\n"]}]},{"cell_type":"code","source":["#4th cs\n","import nltk\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Initialize NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Function to perform semantic analysis\n","def semantic_analysis(text):\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n","    synonyms = set()\n","    for token in lemmatized_tokens:\n","        for syn in wordnet.synsets(token):\n","            for lemma in syn.lemmas():\n","                synonyms.add(lemma.name())\n","    return list(synonyms)\n","\n","# Example customer queries\n","customer_queries = [\n","    \"I received a damaged product. Can I get a refund?\",\n","    \"I'm having trouble accessing my account.\",\n","    \"How can I track my order status?\",\n","    \"The item I received doesn't match the description.\",\n","    \"Is there a discount available for bulk orders?\"\n","]\n","\n","# Semantic analysis for each query\n","for query in customer_queries:\n","    print(\"Customer Query:\", query)\n","    synonyms = semantic_analysis(query)\n","    print(\"Semantic Analysis (Synonyms):\", synonyms)\n","    print(\"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Upsc5XlPjhN1","executionInfo":{"status":"ok","timestamp":1714020118458,"user_tz":-330,"elapsed":3810,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"be5aaa98-424a-4f1a-a75c-fb4b0266b702"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Customer Query: I received a damaged product. Can I get a refund?\n","Semantic Analysis (Synonyms): ['pay_back', 'damaged', 'stupefy', 'gravel', 'experience', 'convey', 'sustain', 'invite', 'grow', 'begin', 'stick', 'contract', 'buzz_off', 'amaze', 'fix', 'product', 'fetch', 'receive', 'develop', 'induce', 'cause', 'get_down', 'go', 'make', 'draw', 'repay', 'encounter', 'sire', 'drive', 'get', 'discredited', 'aim', 'generate', 'puzzle', 'scram', 'beat', 'baffle', 'intersection', 'pick_up', 'Cartesian_product', 'come', 'mathematical_product', 'catch', 'become', 'merchandise', 'received', 'give_back', 'production', \"get_under_one's_skin\", 'stimulate', 'start_out', 'mother', 'pose', 'take_in', 'set_out', 'ware', 'suffer', 'father', 'bewilder', 'return', 'flummox', 'welcome', 'let', 'repayment', 'bring_forth', 'arrest', 'incur', 'vex', 'nonplus', 'mystify', 'beget', 'commence', 'bugger_off', 'engender', 'find', 'dumbfound', 'start', 'produce', 'standard', 'take', 'acquire', 'pay_off', 'arrive', 'meet', 'set_about', 'refund', 'have', 'damage', 'bring', 'perplex', 'capture', 'obtain', 'fuck_off']\n","\n","\n","Customer Query: I'm having trouble accessing my account.\n","Semantic Analysis (Synonyms): ['chronicle', 'inconvenience', 'fuss', 'discommode', 'trouble_oneself', 'history', 'worry', 'answer_for', 'score', 'hassle', 'incommode', 'disturb', 'put_out', 'access', 'trouble', 'bother', 'describe', 'account_statement', 'bill', 'inconvenience_oneself', 'pain', 'business_relationship', 'account', 'report', 'difficulty', 'explanation', 'disquiet', 'calculate', 'invoice', 'problem', 'disoblige', 'distract', 'cark', 'ail', 'accounting', 'get_at', 'news_report', 'unhinge', 'upset', 'disorder', 'perturb', 'write_up', 'story']\n","\n","\n","Customer Query: How can I track my order status?\n","Semantic Analysis (Synonyms): ['orderliness', 'chase_after', 'racetrack', 'ordain', 'purchase_order', 'get_over', 'path', 'tag', 'society', 'edict', 'track', 'order', 'lodge', 'social_club', 'arrange', 'guild', 'pass_over', 'cut_across', 'parliamentary_law', 'put', 'caterpillar_tread', 'Holy_Order', 'regulate', 'grade', 'tell', 'data_track', 'caterpillar_track', 'runway', 'rank', 'regularize', 'rules_of_order', 'fiat', 'consecrate', 'raceway', 'dictate', 'course', 'get_across', 'racecourse', 'cover', 'condition', 'ordinate', 'ordering', 'ordination', 'rail', 'say', 'traverse', 'place', 'give_chase', 'lead', 'gild', 'tail', 'status', 'monastic_order', 'position', 'go_after', 'Order', 'prescribe', 'rate', 'set_up', 'range', 'regularise', 'rescript', 'cart_track', 'decree', 'cut_through', 'running', 'trail', 'chase', 'dog', 'cut', 'parliamentary_procedure', 'club', 'order_of_magnitude', 'govern', 'rails', 'cross', 'cartroad', 'enjoin']\n","\n","\n","Customer Query: The item I received doesn't match the description.\n","Semantic Analysis (Synonyms): ['fit', 'find', 'received', 'experience', 'standard', 'equate', 'touch', 'play_off', 'particular', 'peer', 'encounter', 'item', 'lucifer', 'equalise', 'pair', 'get', 'oppose', 'jibe', 'point', 'description', 'detail', 'verbal_description', 'mates', 'equalize', 'take_in', 'invite', 'couple', 'gibe', 'rival', 'meet', 'cope_with', 'twin', 'tally', 'have', 'equal', 'match', 'agree', 'receive', 'token', 'friction_match', 'mate', 'pit', 'check', 'pick_up', 'welcome', 'compeer', 'incur', 'obtain', 'correspond', 'catch']\n","\n","\n","Customer Query: Is there a discount available for bulk orders?\n","Semantic Analysis (Synonyms): ['orderliness', 'Holy_Order', 'regulate', 'uncommitted', 'grade', 'tell', 'ordain', 'decree', 'price_reduction', 'purchase_order', 'ordinate', 'bulk', 'ordering', 'volume', 'bulge', 'ordination', 'discount_rate', 'usable', 'say', 'discount', 'society', 'useable', 'rank', 'deduction', 'brush_off', 'disregard', 'edict', 'order', 'lodge', 'majority', 'social_club', 'place', 'arrange', 'parliamentary_procedure', 'regularize', 'dismiss', 'rules_of_order', 'gild', 'club', 'brush_aside', 'bank_discount', 'order_of_magnitude', 'monastic_order', 'available', 'fiat', 'guild', 'consecrate', 'dictate', 'govern', 'Order', 'prescribe', 'parliamentary_law', 'set_up', 'rate', 'push_aside', 'range', 'enjoin', 'rebate', 'rescript', 'regularise', 'put', 'mass', 'ignore']\n","\n","\n"]}]},{"cell_type":"code","source":["#5\n","# Install necessary libraries\n","!pip install scikit-learn\n","!pip install nltk\n","# Import required libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.corpus import movie_reviews # Sample dataset from NLTK\n","# Download NLTK resources (run only once if not downloaded)\n","import nltk\n","nltk.download('movie_reviews')\n","# Load the movie_reviews dataset\n","documents = [(list(movie_reviews.words(fileid)), category)\n","for category in movie_reviews.categories()\n","for fileid in movie_reviews.fileids(category)]\n","# Convert data to DataFrame\n","df = pd.DataFrame(documents, columns=['text', 'sentiment'])\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2,\n","random_state=42)\n","# Initialize TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","# Fit and transform the training data\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.apply(' '.join))\n","# Initialize SVM classifier\n","svm_classifier = SVC(kernel='linear')\n","# Train the classifier\n","svm_classifier.fit(X_train_tfidf, y_train)\n","# Transform the test data\n","X_test_tfidf = tfidf_vectorizer.transform(X_test.apply(' '.join))\n","# Predict on the test data\n","y_pred = svm_classifier.predict(X_test_tfidf)\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy:.2f}')\n","# Display classification report\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LE-v_yfUj_gs","executionInfo":{"status":"ok","timestamp":1714020205749,"user_tz":-330,"elapsed":48615,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"afbebe8e-47a5-448a-92e8-5f58b847b3da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.84\n","              precision    recall  f1-score   support\n","\n","         neg       0.83      0.85      0.84       199\n","         pos       0.85      0.82      0.84       201\n","\n","    accuracy                           0.84       400\n","   macro avg       0.84      0.84      0.84       400\n","weighted avg       0.84      0.84      0.84       400\n","\n"]}]},{"cell_type":"code","source":["#5th cs\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# Download NLTK resources (only required once)\n","nltk.download('vader_lexicon')\n","\n","# Sample reviews\n","reviews = [\n","    \"This product is amazing! I love it.\",\n","    \"The product was good, but the packaging was damaged.\",\n","    \"Very disappointing experience. Would not recommend.\",\n","    \"Neutral feedback on the product.\",\n","]\n","\n","# Initialize Sentiment Intensity Analyzer\n","sid = SentimentIntensityAnalyzer()\n","\n","# Analyze sentiment for each review\n","for review in reviews:\n","    print(\"Review:\", review)\n","    scores = sid.polarity_scores(review)\n","    print(\"Sentiment:\", end=' ')\n","    if scores['compound'] > 0.05:\n","        print(\"Positive\")\n","    elif scores['compound'] < -0.05:\n","        print(\"Negative\")\n","    else:\n","        print(\"Neutral\")\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFvL8LrNkYlC","executionInfo":{"status":"ok","timestamp":1714020292741,"user_tz":-330,"elapsed":390,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"756730c5-a624-4b01-b3a8-7d44b92e6a97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: This product is amazing! I love it.\n","Sentiment: Positive\n","\n","Review: The product was good, but the packaging was damaged.\n","Sentiment: Negative\n","\n","Review: Very disappointing experience. Would not recommend.\n","Sentiment: Negative\n","\n","Review: Neutral feedback on the product.\n","Sentiment: Neutral\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["#6th\n","# Install NLTK (if not already installed)\n","!pip install nltk\n","# Import necessary libraries\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","# Sample text for POS tagging\n","text = \"Parts of speech tagging helps to understand the function of each word in a sentence.\"\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(tokens)\n","# Display the POS tags\n","print(\"POS tags:\", pos_tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIuOZajmklpN","executionInfo":{"status":"ok","timestamp":1714020338365,"user_tz":-330,"elapsed":15838,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"d22c54d2-a334-427b-90fb-c563cc3ef53b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["POS tags: [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'VBG'), ('helps', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('function', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["###6th cs###\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# Download NLTK resources (if not already downloaded)\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","def pos_tagging(text):\n","    sentences = sent_tokenize(text)\n","    tagged_tokens = []\n","    for sentence in sentences:\n","        tokens = word_tokenize(sentence)\n","        tagged_tokens.extend(nltk.pos_tag(tokens))\n","    return tagged_tokens\n","\n","def main():\n","    article_text = \"\"\"Manchester United secured a 3-1 victory over Chelsea in yesterday's match.\n","    Goals from Rashford, Greenwood, and Fernandes sealed the win for United.\n","    Chelsea's only goal came from Pulisic in the first half.\n","    The victory boosts United's chances in the Premier League title race.\n","    \"\"\"\n","    tagged_tokens = pos_tagging(article_text)\n","    print(\"Original Article Text:\\n\", article_text)\n","    print(\"\\nParts of Speech Tagging:\")\n","    for token, pos_tag in tagged_tokens:\n","        print(f\"{token}: {pos_tag}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbx2-0qvk2tO","executionInfo":{"status":"ok","timestamp":1714020432065,"user_tz":-330,"elapsed":396,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"5b5ebea5-6b79-4036-ca95-cd0de41c2a24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Article Text:\n"," Manchester United secured a 3-1 victory over Chelsea in yesterday's match.\n","    Goals from Rashford, Greenwood, and Fernandes sealed the win for United.\n","    Chelsea's only goal came from Pulisic in the first half.\n","    The victory boosts United's chances in the Premier League title race.\n","    \n","\n","Parts of Speech Tagging:\n","Manchester: NNP\n","United: NNP\n","secured: VBD\n","a: DT\n","3-1: JJ\n","victory: NN\n","over: IN\n","Chelsea: NNP\n","in: IN\n","yesterday: NN\n","'s: POS\n","match: NN\n",".: .\n","Goals: NNS\n","from: IN\n","Rashford: NNP\n",",: ,\n","Greenwood: NNP\n",",: ,\n","and: CC\n","Fernandes: NNP\n","sealed: VBD\n","the: DT\n","win: NN\n","for: IN\n","United: NNP\n",".: .\n","Chelsea: NN\n","'s: POS\n","only: JJ\n","goal: NN\n","came: VBD\n","from: IN\n","Pulisic: NNP\n","in: IN\n","the: DT\n","first: JJ\n","half: NN\n",".: .\n","The: DT\n","victory: NN\n","boosts: VBZ\n","United: NNP\n","'s: POS\n","chances: NNS\n","in: IN\n","the: DT\n","Premier: NNP\n","League: NNP\n","title: NN\n","race: NN\n",".: .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["#7th\n","!pip install nltk\n","import nltk\n","from nltk import RegexpParser\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","\n","# Download NLTK resources (run only once if not downloaded)\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample sentence\n","sentence = \"The quick brown fox jumps over the lazy dog\"\n","\n","# Tokenize the sentence\n","tokens = word_tokenize(sentence)\n","\n","# POS tagging\n","tagged = pos_tag(tokens)\n","\n","# Define a chunk grammar using regular expressions\n","# NP (noun phrase) chunking: \"NP: {<DT>?<JJ>*<NN>}\"\n","# This grammar captures optional determiner (DT), adjectives (JJ), and nouns (NN) as a noun phrase\n","chunk_grammar = r\"\"\"\n","NP: {<DT>?<JJ>*<NN>}\n","\"\"\"\n","\n","# Create a chunk parser with the defined grammar\n","chunk_parser = RegexpParser(chunk_grammar)\n","\n","# Parse the tagged sentence to extract chunks\n","chunks = chunk_parser.parse(tagged)\n","\n","# Display the chunks\n","for subtree in chunks.subtrees():\n","    if subtree.label() == 'NP':\n","        print(subtree)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kppMS1mjlO2K","executionInfo":{"status":"ok","timestamp":1714020556637,"user_tz":-330,"elapsed":12339,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"817e420f-7c4a-4d20-c96a-c7b484274c04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","(NP The/DT quick/JJ brown/NN)\n","(NP fox/NN)\n","(NP the/DT lazy/JJ dog/NN)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["import nltk\n","import os\n","\n","# Set NLTK data path\n","nltk.data.path.append(\"/usr/local/share/nltk_data\")\n","\n","# Download the 'punkt' tokenizer model\n","nltk.download('punkt')\n","\n","# Download the 'averaged_perceptron_tagger' model\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the text into words\n","words = nltk.word_tokenize(text)\n","\n","# Perform part-of-speech tagging\n","pos_tags = nltk.pos_tag(words)\n","\n","# Define chunk grammar\n","chunk_grammar = r\"\"\"\n","NP: {<DT>?<JJ>*<NN>} # Chunk sequences of DT, JJ, NN\n","\"\"\"\n","\n","# Create chunk parser\n","chunk_parser = nltk.RegexpParser(chunk_grammar)\n","\n","# Apply chunking\n","chunked_text = chunk_parser.parse(pos_tags)\n","\n","# Extract noun phrases\n","noun_phrases = []\n","for subtree in chunked_text.subtrees(filter=lambda t: t.label() == 'NP'):\n","    noun_phrases.append(' '.join(word for word, tag in subtree.leaves()))\n","\n","# Output\n","print(\"Original Text:\", text)\n","print(\"Noun Phrases:\")\n","for phrase in noun_phrases:\n","    print(\"-\", phrase)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8c-U7cg0oqFz","executionInfo":{"status":"ok","timestamp":1714021381119,"user_tz":-330,"elapsed":373,"user":{"displayName":"Janardhan Sanivarapu","userId":"17909985261217253917"}},"outputId":"e0d3dbbc-2578-421f-8268-04e05938b3b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: The quick brown fox jumps over the lazy dog.\n","Noun Phrases:\n","- The quick brown\n","- fox\n","- the lazy dog\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}